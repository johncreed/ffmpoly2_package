\section{Introduction}

Recommendation has received much attention from both research and industry. The prediction of click-through rate (CTR) provides an important solution for recommending items to users. For example, some recommender systems require maximizing the number of clicks on the list of recommended items, a task that can be accomplished by ranking items according to CTR. On the other hand, online advertising systems aim to improve the revenue, which can be estimated as the CTR prediction multiplied by the received bid. Accurate CTR prediction is therefore essential for solving certain recommendation problems.

Some machine learning models have been applied in CTR prediction. Among them, logistic regression and its extensions have been widely used for this task \cite{Richardson:2007:PCE:1242572.1242643}. Suppose the data set consists of $m$ instances $\lbrace\boldsymbol{x}_i,y_i\rbrace_{i=1}^m$, where $\boldsymbol{x}_i$ is a data instance including $n$ user and item attributes, and $y_i\in\lbrace-1,1\rbrace$ is the associated label. That is, $y_i=1$ means the user clicks the item, and $y_i=-1$ otherwise. For any data point $\boldsymbol{x}$, if  $\boldsymbol{w}$ is the model vector and $\phi(\boldsymbol{w}, \boldsymbol{x})$ is the prediction function on whether $\boldsymbol{x}$ is clicked, logistic regression solves the following optimization problem:

\begin{equation}\label{1.1}
\min_{\boldsymbol{w}} \frac{\lambda}{2} \|\boldsymbol{w}\|_2^2+\sum_{i=1}^{m}\log(1+\exp(-y_i\phi(\boldsymbol{w},\boldsymbol{x}_i))),
\end{equation}
where $\lambda$ is the regularization parameter and the $\|\boldsymbol{w}\|_2^2$ term is used to avoid overfitting data. The second term in $(\ref{1.1})$ is an approximate sum of training errors by using the logistic loss.
In problem $(\ref{1.1})$, the $\phi$ function plays a very important role. Traditionally, a linear model (LM) is considered with $\phi(\boldsymbol{w}, \boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}$. In a linear model, every feature corresponds to an individual weight, so relationship between features is not taken into account. However, as we observe in, for example, a commercial APP market, current app id combined with past downloaded apps is an important indicator in the prediction. Therefore, feature pair conjunction is crucial in CTR prediction and many works in real recommender systems are devoted to feature engineering, which aims to find effective feature interactions.

In data classification, second-order models are usually introduced to learn the feature conjunction instead of artificial feature interactions. As a widely adopted model, degree-2 polynomial (Poly2) \cite{Chang:2010:TTL:1756006.1859899} learns a particular weight for each possible feature conjunction. However, it has been known that Poly2 is not suitable for sparse data \cite{Juan:2016:FFM:2959100.2959134}, which are very common in real-world applications. Factorization machine (FM) \cite{5694074}\cite{Rendle:2012:FML:2168752.2168771} hence provides another way of feature conjunction under the sparse setting, which decomposes Poly2's one weight for each feature pair into the inner product of two short latent vectors associated with the feature pair. Furthermore, field-aware factorization machine (FFM) \cite{Juan:2016:FFM:2959100.2959134} has been proposed based on FM. FFM introduces the concept of fields, so feature fields are taken into consideration when combining a feature pair. All these models perform better than LM, indicating that feature conjunction is a key factor in predicting the CTR score.

It has been shown \cite{Juan:2016:FFM:2959100.2959134} that Poly2 and FM/FFM are useful for different types of data. In addition, one key observation we made is that the frequency of feature pairs in real data sets is not consistent, indicating that a single model cannot handle all real data sets. Therefore, having a model able to adapt to the data set seems to be essential for obtaining good performances.

In this paper, we propose a data adaptive framework to address the feature conjunction problem. We frame the problem as a model-based feature conjunction challenge. To find an appropriate feature conjunction for a feature pair, our framework adaptively chooses effective models based on data properties. As a case study, we then offer a novel adaptive model for building feature conjunctions in terms of the feature-pair frequency. Moreover, we search for the best hyperparameters to achieve the best adaptive setting. With the adaptive model, we significantly improve the performance of CTR prediction on multiple real-world data sets.

Our main contributions are summarized as follows:
\begin{itemize}
\item We propose a novel adaptive feature conjunction framework, consisting of the combination of several models. We specify the components of this framework, and justify how this framework works.
\item We offer a case study of the proposed framework by integrating Poly2 and FFM based on the feature-pair frequency. Moreover, we offer an effective optimization algorithm and provide a guideline on how to adjust the optimal hyperparameters in the model.
\item We evaluate our model on both public and our production data. Results show that our model gains improvement over existing models for CTR predcition.
\end{itemize}
